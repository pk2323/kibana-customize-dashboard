{"remainingRequest":"C:\\Users\\Prashanth\\Desktop\\ELk\\kibana-6.2.4\\kibana-6.2.4\\node_modules\\babel-loader\\lib\\index.js??ref--6-1!C:\\Users\\Prashanth\\Desktop\\ELk\\kibana-6.2.4\\kibana-6.2.4\\src\\core_plugins\\console\\public\\tests\\src\\output_tokenization_tests.js","dependencies":[{"path":"C:\\Users\\Prashanth\\Desktop\\ELk\\kibana-6.2.4\\kibana-6.2.4\\src\\core_plugins\\console\\public\\tests\\src\\output_tokenization_tests.js","mtime":1525446212594},{"path":"C:\\Users\\Prashanth\\Desktop\\ELk\\kibana-6.2.4\\kibana-6.2.4\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1493198456000},{"path":"C:\\Users\\Prashanth\\Desktop\\ELk\\kibana-6.2.4\\kibana-6.2.4\\node_modules\\babel-loader\\lib\\index.js","mtime":1503096278000}],"contextDependencies":[],"result":["'use strict';\n\nvar _output = require('../../src/output');\n\nvar ace = require('ace');\nvar $ = require('jquery');\nvar RowParser = require('../../src/sense_editor/row_parser');\n\nvar output = void 0;\n\nvar token_iterator = ace.require(\"ace/token_iterator\");\nvar _window$QUnit = window.QUnit,\n    _module = _window$QUnit.module,\n    asyncTest = _window$QUnit.asyncTest,\n    deepEqual = _window$QUnit.deepEqual,\n    start = _window$QUnit.start;\n\n\n_module(\"Output Tokenization\", {\n  setup: function setup() {\n    output = (0, _output.initializeOutput)($('#output'));\n    output.$el.show();\n  },\n  teardown: function teardown() {\n    output.$el.hide();\n  }\n});\n\nfunction tokensAsList() {\n  var iter = new token_iterator.TokenIterator(output.getSession(), 0, 0);\n  var ret = [];\n  var t = iter.getCurrentToken();\n  var parser = new RowParser(output);\n  if (parser.isEmptyToken(t)) {\n    t = parser.nextNonEmptyToken(iter);\n  }\n  while (t) {\n    ret.push({ value: t.value, type: t.type });\n    t = parser.nextNonEmptyToken(iter);\n  }\n\n  return ret;\n}\n\nvar testCount = 0;\n\nfunction token_test(token_list, data) {\n  if (data && typeof data != \"string\") {\n    data = JSON.stringify(data, null, 3);\n  }\n\n  asyncTest(\"Token test \" + testCount++, function () {\n    output.update(data, function () {\n      var tokens = tokensAsList();\n      var normTokenList = [];\n      for (var i = 0; i < token_list.length; i++) {\n        normTokenList.push({ type: token_list[i++], value: token_list[i] });\n      }\n\n      deepEqual(tokens, normTokenList, \"Doc:\\n\" + data);\n      start();\n    });\n  });\n}\n\ntoken_test([\"warning\", \"#! warning\", \"comment\", \"# GET url\", \"paren.lparen\", \"{\", \"paren.rparen\", \"}\"], \"#! warning\\n\" + \"# GET url\\n\" + \"{}\");\n\ntoken_test([\"comment\", \"# GET url\", \"paren.lparen\", \"{\", \"variable\", '\"f\"', \"punctuation.colon\", \":\", \"punctuation.start_triple_quote\", '\"\"\"', \"multi_string\", \"raw\", \"punctuation.end_triple_quote\", '\"\"\"', \"paren.rparen\", \"}\"], '# GET url\\n' + '{ \"f\": \"\"\"raw\"\"\" }');",null]}